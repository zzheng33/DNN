{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b18646-1090-4707-a04d-b8a4e39d8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet101\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import Subset\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "def get_random_subset_indices(num_samples, dataset_size):\n",
    "    return random.sample(range(dataset_size), num_samples)\n",
    "\n",
    "\n",
    "def create_dataset():\n",
    "     # Data augmentation and normalization for training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    train_dataset = torchvision.datasets.ImageFolder(os.path.join(\"/lus/eagle/projects/datascience/ImageNet/ILSVRC/Data/CLS-LOC\", \"train\"), transform=train_transform)\n",
    "\n",
    "    num_samples = 10000\n",
    "\n",
    "    train_indices = get_random_subset_indices(num_samples, len(train_dataset))\n",
    "    small_train_dataset = Subset(train_dataset, train_indices)\n",
    "    return small_train_dataset\n",
    "\n",
    "def build_model():\n",
    "    model = resnet101(weights=None)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    return model, criterion, optimizer, device\n",
    "\n",
    "def create_dataLoader(batch_size=256, workers=8):\n",
    "    return DataLoader(create_dataset(), batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "\n",
    "def select_device(selected_gpus):\n",
    "     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, selected_gpus))\n",
    "\n",
    "def train_one_epoch(model, criterion, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    \n",
    "    total_images = 0\n",
    "    start_time = time.time()\n",
    "    start_time_dataLoad = time.time()\n",
    "    end_time_dataLoad = 0\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_images += inputs.size(0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    images_per_second = total_images / (end_time - start_time)\n",
    "    dataLoad_time = end_time_dataLoad - start_time_dataLoad\n",
    "    return int(images_per_second), end_time-start_time\n",
    "\n",
    "def train(batch_size=256, GPU_selection=[0, 1], epoch=5, num_workers=8):\n",
    "    \n",
    "    num_epochs = epoch  # Adjust this value according to your needs\n",
    "    train_loader = create_dataLoader(batch_size, num_workers)\n",
    "    select_device(GPU_selection)\n",
    "    model,criterion,optimizer,device = build_model()\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        images_per_second, epoch_duration = train_one_epoch(model, criterion, optimizer, train_loader, device)\n",
    "      \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}],Duration: {epoch_duration:.2f}s, Images/s: {images_per_second}\")\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ea178d-4ee3-4b0d-a1b0-5aa9bf854701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5],Duration: 36.19s, Images/s: 276\n",
      "Epoch [2/5],Duration: 11.13s, Images/s: 898\n",
      "Epoch [3/5],Duration: 11.13s, Images/s: 898\n",
      "Epoch [4/5],Duration: 11.10s, Images/s: 901\n",
      "Epoch [5/5],Duration: 11.01s, Images/s: 908\n"
     ]
    }
   ],
   "source": [
    "train(batch_size=256, GPU_selection=[0, 1], epoch=5, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521837e5-4a9f-4ed7-ae09-149324e4b488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience/conda-2023-01-10",
   "language": "python",
   "name": "conda-2023-01-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
